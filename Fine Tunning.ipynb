{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8686327,"sourceType":"datasetVersion","datasetId":5207951},{"sourceId":8700107,"sourceType":"datasetVersion","datasetId":5218000},{"sourceId":8700230,"sourceType":"datasetVersion","datasetId":5218058},{"sourceId":8700259,"sourceType":"datasetVersion","datasetId":5218076}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import MarianMTModel, MarianTokenizer, TrainingArguments, Trainer, Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\nfrom datasets import DatasetDict , load_dataset, Dataset, concatenate_datasets\nimport os \n#import evaluate\nimport pandas as pd\nos.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"execution":{"iopub.status.busy":"2024-06-17T10:26:58.201374Z","iopub.execute_input":"2024-06-17T10:26:58.201727Z","iopub.status.idle":"2024-06-17T10:27:15.299945Z","shell.execute_reply.started":"2024-06-17T10:26:58.201698Z","shell.execute_reply":"2024-06-17T10:27:15.299181Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-06-17 10:27:05.614823: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-17 10:27:05.614923: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-17 10:27:05.745270: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"adjectives = pd.read_csv(\"/kaggle/input/asjectives/adjectives.csv\")\nadverbs = pd.read_csv(\"/kaggle/input/adverbs/adverbs.csv\")\nprepositions = pd.read_csv(\"/kaggle/input/prepostionnouns/prepositions.csv\")\nnouns = pd.read_csv(\"/kaggle/input/prepostionnouns/prepositions.csv\")\n\n\nadjectives = adjectives.rename(columns={\"n1\": \"darija\", \"eng\":\"en\"})\nprepositions = prepositions.rename(columns={\"n1\": \"darija\", \"eng\":\"en\"})\nnouns = nouns.rename(columns={\"n1\": \"darija\", \"eng\":\"en\"})\nadverbs = adverbs.rename(columns={\"n1\": \"darija\", \"eng\":\"en\"})\n\ndataset_adjectives = Dataset.from_pandas(adjectives)\ndataset_prepositions = Dataset.from_pandas(prepositions)\ndataset_nouns = Dataset.from_pandas(nouns)\ndataset_adverbs = Dataset.from_pandas(adverbs)\n\nprint(len(adjectives) + len(adverbs) + len(prepositions) + len(nouns))","metadata":{"execution":{"iopub.status.busy":"2024-06-17T10:27:15.301549Z","iopub.execute_input":"2024-06-17T10:27:15.302088Z","iopub.status.idle":"2024-06-17T10:27:15.469225Z","shell.execute_reply.started":"2024-06-17T10:27:15.302061Z","shell.execute_reply":"2024-06-17T10:27:15.468369Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"1182\n","output_type":"stream"}]},{"cell_type":"code","source":"dataset_ar_eng1 = load_dataset(\"atlasia/darija-translation\")\ndataset_ar_eng2 = load_dataset(\"BounharAbdelaziz/English-to-Moroccan-Darija\")\ndataset_ar_eng2 = dataset_ar_eng2.rename_column(\"english\", \"en\")\n\n\ndataset_ar_eng = concatenate_datasets([dataset_ar_eng1[\"train\"], dataset_ar_eng2[\"train\"] ,dataset_adjectives, dataset_adverbs, dataset_nouns, dataset_prepositions])\ndataset_ar_eng = dataset_ar_eng.train_test_split(test_size=0.2)\n\n# Filter out rows with NaN values\ndataset_ar_eng = DatasetDict({\n    'train': dataset_ar_eng['train'].filter(lambda x: x['darija'] is not None and x['en'] is not None),\n    'test': dataset_ar_eng['test'].filter(lambda x: x['darija'] is not None and x['en'] is not None)\n})\n\n\n\ndataset_ar_eng = DatasetDict({\n    'train': dataset_ar_eng['train'].remove_columns([col for col in dataset_ar_eng['train'].column_names if col not in ['darija', 'en']]),\n    'test': dataset_ar_eng['test'].remove_columns([col for col in dataset_ar_eng['test'].column_names if col not in ['darija', 'en']])\n})\n\ndataset_ar_eng","metadata":{"execution":{"iopub.status.busy":"2024-06-17T10:27:15.470431Z","iopub.execute_input":"2024-06-17T10:27:15.471058Z","iopub.status.idle":"2024-06-17T10:27:38.451807Z","shell.execute_reply.started":"2024-06-17T10:27:15.471024Z","shell.execute_reply":"2024-06-17T10:27:38.450886Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/2.55k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0fea77b1bb6c44169a73d980befc40a9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/2.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3a00c3cb95f49a7a4bfc45cb9c5e2c0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/45103 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3361232b762c46d996761f48fdd81b86"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91f14df3f75a4069964cacc08dee5610"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/1.70M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7af49ecb4d74b8580129afd661ac65e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/16089 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ed8b201dea44e8594aca8981fd63e2b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/49899 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae177cbfcb72429188bff3ad2bcfc246"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/12475 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3772bfe4757048a58d00c0d3f8e4ffdc"}},"metadata":{}},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['darija', 'en'],\n        num_rows: 49578\n    })\n    test: Dataset({\n        features: ['darija', 'en'],\n        num_rows: 12407\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"model_name = \"marefa-nlp/marefa-mt-en-ar\"\ntokinezer = MarianTokenizer.from_pretrained(model_name)\nmodel = MarianMTModel.from_pretrained(model_name)\n\ndef process_data(examples):\n    prefix = \"translate English to arabic: \"\n    inputs = [prefix + example for example in examples[\"en\"]]\n    targets = [example for example in examples['darija']]\n    \n    model_inputs = tokinezer(inputs, max_length=128, truncation=True, padding=\"max_length\")\n    \n    with tokinezer.as_target_tokenizer():\n        labels = tokinezer(targets, max_length=128, truncation=True, padding=\"max_length\")\n        \n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs","metadata":{"execution":{"iopub.status.busy":"2024-06-17T10:27:38.454141Z","iopub.execute_input":"2024-06-17T10:27:38.454960Z","iopub.status.idle":"2024-06-17T10:27:58.474589Z","shell.execute_reply.started":"2024-06-17T10:27:38.454932Z","shell.execute_reply":"2024-06-17T10:27:58.473618Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/237 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa00b66ff7634913a76cd061e569ebf4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"source.spm:   0%|          | 0.00/801k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e6de334112846f1b47979842922a160"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"target.spm:   0%|          | 0.00/917k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d4f0e5dfd68479eb97e72794800eb94"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/2.24M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"243c4edc28f94551aae177b56628bb9f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/65.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"480b3420ae8c4820a9d01bc457de29e3"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.32k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c89fd0b7dd3f4b06adf036f01a69c4bf"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n  warnings.warn(\"Recommended: pip install sacremoses.\")\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/306M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33d437e22fc34510928dca7f0c53a521"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n","output_type":"stream"}]},{"cell_type":"code","source":"tokinezed_dataset_ar_en = dataset_ar_eng.map(process_data, batched=True, remove_columns=dataset_ar_eng[\"train\"].column_names)","metadata":{"execution":{"iopub.status.busy":"2024-06-17T10:27:58.475876Z","iopub.execute_input":"2024-06-17T10:27:58.476259Z","iopub.status.idle":"2024-06-17T10:28:20.605827Z","shell.execute_reply.started":"2024-06-17T10:27:58.476222Z","shell.execute_reply":"2024-06-17T10:28:20.604872Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/49578 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a143bd7803a46ec839cff52e3bb6af3"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3946: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/12407 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab2bd19f47334222ab94318ab33fc918"}},"metadata":{}}]},{"cell_type":"code","source":"data_collator = DataCollatorForSeq2Seq(tokinezer, model=model)","metadata":{"execution":{"iopub.status.busy":"2024-06-17T10:28:28.581475Z","iopub.execute_input":"2024-06-17T10:28:28.581830Z","iopub.status.idle":"2024-06-17T10:28:28.586124Z","shell.execute_reply.started":"2024-06-17T10:28:28.581803Z","shell.execute_reply":"2024-06-17T10:28:28.585228Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def compute_metrics(eval_preds):\n    preds, labels = eval_preds\n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n    \n    # BLEU score\n    bleu_result = bleu.compute(predictions=decoded_preds, references=decoded_labels)\n    \n    # ROUGE score\n    rouge_result = rouge.compute(predictions=decoded_preds, references=decoded_labels)\n    \n    # Combine metrics\n    result = {\n        \"bleu\": bleu_result[\"bleu\"],\n        \"rouge1\": rouge_result[\"rouge1\"].mid.fmeasure,\n        \"rouge2\": rouge_result[\"rouge2\"].mid.fmeasure,\n        \"rougeL\": rouge_result[\"rougeL\"].mid.fmeasure,\n        \"rougeLsum\": rouge_result[\"rougeLsum\"].mid.fmeasure,\n    }\n    \n    return result","metadata":{"execution":{"iopub.status.busy":"2024-06-17T12:34:11.399153Z","iopub.execute_input":"2024-06-17T12:34:11.400061Z","iopub.status.idle":"2024-06-17T12:34:11.407120Z","shell.execute_reply.started":"2024-06-17T12:34:11.400028Z","shell.execute_reply":"2024-06-17T12:34:11.406258Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"training_args = Seq2SeqTrainingArguments(\n    output_dir=\"./results\",\n    evaluation_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=32,\n    weight_decay=0.01,\n    save_total_limit=3,\n    num_train_epochs=6,\n    fp16=True,\n)\n\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokinezed_dataset_ar_en[\"train\"],\n    eval_dataset=tokinezed_dataset_ar_en[\"test\"],\n    tokenizer=tokinezer,\n    data_collator=data_collator,\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-17T10:28:31.154827Z","iopub.execute_input":"2024-06-17T10:28:31.155643Z","iopub.status.idle":"2024-06-17T10:28:31.426419Z","shell.execute_reply.started":"2024-06-17T10:28:31.155613Z","shell.execute_reply":"2024-06-17T10:28:31.425637Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-06-17T10:28:31.510904Z","iopub.execute_input":"2024-06-17T10:28:31.511219Z","iopub.status.idle":"2024-06-17T11:46:41.322743Z","shell.execute_reply.started":"2024-06-17T10:28:31.511180Z","shell.execute_reply":"2024-06-17T11:46:41.321849Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='9300' max='9300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [9300/9300 1:18:08, Epoch 6/6]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.398500</td>\n      <td>0.356238</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.330500</td>\n      <td>0.307416</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.297300</td>\n      <td>0.282857</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.276900</td>\n      <td>0.269981</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.263200</td>\n      <td>0.262747</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.256800</td>\n      <td>0.260168</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[62801]], 'forced_eos_token_id': 0}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[62801]], 'forced_eos_token_id': 0}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[62801]], 'forced_eos_token_id': 0}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[62801]], 'forced_eos_token_id': 0}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[62801]], 'forced_eos_token_id': 0}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[62801]], 'forced_eos_token_id': 0}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[62801]], 'forced_eos_token_id': 0}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[62801]], 'forced_eos_token_id': 0}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[62801]], 'forced_eos_token_id': 0}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[62801]], 'forced_eos_token_id': 0}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[62801]], 'forced_eos_token_id': 0}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[62801]], 'forced_eos_token_id': 0}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[62801]], 'forced_eos_token_id': 0}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[62801]], 'forced_eos_token_id': 0}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[62801]], 'forced_eos_token_id': 0}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[62801]], 'forced_eos_token_id': 0}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[62801]], 'forced_eos_token_id': 0}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[62801]], 'forced_eos_token_id': 0}\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=9300, training_loss=0.33730128462596604, metrics={'train_runtime': 4689.5066, 'train_samples_per_second': 63.433, 'train_steps_per_second': 1.983, 'total_flos': 1.0083678818402304e+16, 'train_loss': 0.33730128462596604, 'epoch': 6.0})"},"metadata":{}}]},{"cell_type":"code","source":"# English Sample Text\ninput = \"do you realy love me\"\n\ntranslated_tokens = model.generate(**tokinezer.prepare_seq2seq_batch([input], return_tensors=\"pt\").to(\"cuda\"))\ntranslated_text = [tokinezer.decode(t, skip_special_tokens=True) for t in translated_tokens]\n\n# translated Arabic Text\nprint(translated_text)","metadata":{"execution":{"iopub.status.busy":"2024-06-17T12:54:45.151306Z","iopub.execute_input":"2024-06-17T12:54:45.152004Z","iopub.status.idle":"2024-06-17T12:54:45.397473Z","shell.execute_reply.started":"2024-06-17T12:54:45.151971Z","shell.execute_reply":"2024-06-17T12:54:45.396502Z"},"trusted":true},"execution_count":54,"outputs":[{"name":"stdout","text":"['wach katbghini tari9i']\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}